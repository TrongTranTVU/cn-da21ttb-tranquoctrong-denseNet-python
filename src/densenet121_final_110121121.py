# -*- coding: utf-8 -*-
"""Densenet121-Final-110121121

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KqLozm25pt4uqFmmvXjLbwUlbQ0H_8La

## ğŸ“š **Huáº¥n Luyá»‡n MÃ´ HÃ¬nh DenseNet-121

## ğŸ¯ **Má»¥c TiÃªu Äá» TÃ i**
- XÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n **mÃ´ hÃ¬nh DenseNet-121** hoÃ n chá»‰nh tá»« Ä‘áº§u Ä‘á»ƒ thá»±c hiá»‡n nhiá»‡m vá»¥ **phÃ¢n loáº¡i áº£nh** trong táº­p dá»¯ liá»‡u **CIFAR-10**.  
- Thá»±c hiá»‡n toÃ n bá»™ quy trÃ¬nh huáº¥n luyá»‡n má»™t cÃ¡ch chi tiáº¿t, bao gá»“m:  
  - **Táº£i dá»¯ liá»‡u CIFAR-10** vÃ  **tiá»n xá»­ lÃ½**.  
  - **TÄƒng cÆ°á»ng dá»¯ liá»‡u** (*Data Augmentation*).  
  - **PhÃ¢n chia dá»¯ liá»‡u:** Táº­p **Huáº¥n luyá»‡n** vÃ  **Kiá»ƒm tra**.  
- **Thiáº¿t káº¿ mÃ´ hÃ¬nh DenseNet-121:**  
  - KhÃ´ng sá»­ dá»¥ng cÃ¡c thÆ° viá»‡n mÃ´ hÃ¬nh dá»±ng sáºµn.  
  - Tá»± xÃ¢y dá»±ng toÃ n bá»™ cÃ¡c lá»›p nhÆ° **DenseBlock**, **DenseLayer**, vÃ  **Transition Layer**.  
- ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh qua cÃ¡c chá»‰ sá»‘ quan trá»ng:  
  - **Loss** (*HÃ m máº¥t mÃ¡t*).  
  - **Accuracy** (*Äá»™ chÃ­nh xÃ¡c*).  
  - **Thá»i gian huáº¥n luyá»‡n**.  
- **Kiá»ƒm thá»­ mÃ´ hÃ¬nh**:  
  - Cho phÃ©p ngÆ°á»i dÃ¹ng táº£i lÃªn **áº£nh tÃ¹y chá»‰nh** Ä‘á»ƒ nháº­n diá»‡n Ä‘á»‘i tÆ°á»£ng.  
- **PhÃ¢n tÃ­ch vÃ  trá»±c quan hÃ³a káº¿t quáº£:**  
  - Biá»ƒu Ä‘á»“ **Loss** vÃ  **Accuracy**.  
  - So sÃ¡nh giá»¯a táº­p **Train** vÃ  **Validation**.

## ğŸš€ **YÃªu Cáº§u TrÆ°á»›c Khi Báº¯t Äáº§u**
### Äá»ƒ Ä‘áº£m báº£o quÃ¡ trÃ¬nh huáº¥n luyá»‡n nhanh chÃ³ng vÃ  hiá»‡u quáº£, báº¡n cáº§n thá»±c hiá»‡n:
- **Káº¿t ná»‘i Google Colab vá»›i GPU miá»…n phÃ­.**  
- **Kiá»ƒm tra cáº¥u hÃ¬nh GPU**:  
   - Truy cáº­p menu **Runtime (Thá»i gian cháº¡y)**.  
   - Chá»n **Change runtime type (Thay Ä‘á»•i kiá»ƒu mÃ´i trÆ°á»ng)**.  
   - Trong má»¥c **Hardware accelerator (TrÃ¬nh tÄƒng tá»‘c pháº§n cá»©ng)**, chá»n: **T4 GPU**.  
   - Nháº¥n **Save (LÆ°u)** Ä‘á»ƒ hoÃ n táº¥t.

## **1. CÃ i Äáº·t ThÆ° Viá»‡n vÃ  Káº¿t Ná»‘i Google Drive**

# ğŸ“¦ CÃ i Äáº·t vÃ  Káº¿t Ná»‘i Google Drive
Má»¥c tiÃªu:
- CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t (`pytorch-lightning`, `torchmetrics`)
- Káº¿t ná»‘i Google Drive Ä‘á»ƒ lÆ°u trá»¯ mÃ´ hÃ¬nh vÃ  dá»¯ liá»‡u huáº¥n luyá»‡n.

### CÃ¡c bÆ°á»›c thá»±c hiá»‡n:
1. **CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t:**  
   - `pytorch-lightning`: Framework há»— trá»£ huáº¥n luyá»‡n mÃ´ hÃ¬nh sÃ¢u.  
   - `torchmetrics`: Äá»ƒ tÃ­nh cÃ¡c metric nhÆ° Accuracy, F1-score, Recall, v.v.  
2. **Káº¿t ná»‘i Google Drive:**  
   - Drive sáº½ lÆ°u file `.ckpt` cá»§a mÃ´ hÃ¬nh, file `.json` cá»§a lá»‹ch sá»­ huáº¥n luyá»‡n.
"""

# ============================================================
# 1) CÃ€I Äáº¶T VÃ€ Káº¾T Ná»I GOOGLE DRIVE
# ============================================================
!pip install pytorch-lightning --quiet
!pip install torchmetrics --quiet

import os
import json
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# PyTorch vÃ  cÃ¡c thÃ nh pháº§n liÃªn quan
import torch
import torch.nn.functional as F
from torch import nn, optim
from torch.utils.data import DataLoader

# ThÆ° viá»‡n liÃªn quan Ä‘áº¿n xá»­ lÃ½ áº£nh vÃ  dataset
from torchvision import datasets, transforms
from PIL import Image

# Widgets cho Google Colab (Upload file, hiá»ƒn thá»‹ tÆ°Æ¡ng tÃ¡c)
from google.colab import files
import ipywidgets as widgets
from IPython.display import display, clear_output

# Káº¿t ná»‘i Drive
from google.colab import drive
drive.mount('/content/drive')

base_dir = "/content/drive/MyDrive"
if not os.path.exists(base_dir):
    os.makedirs(base_dir)

print("ÄÃ£ mount Google Drive. base_dir =", base_dir)

"""## **2 Khá»Ÿi Táº¡o MÃ´i TrÆ°á»ng PyTorch Lightning**

# âš¡ Khá»Ÿi Táº¡o PyTorch Lightning
Má»¥c tiÃªu:  
- Thiáº¿t láº­p cáº¥u hÃ¬nh Lightning.  
- Äáº£m báº£o reproducibility (láº·p láº¡i káº¿t quáº£ vá»›i cÃ¹ng bá»™ dá»¯ liá»‡u).  

### CÃ¡c bÆ°á»›c thá»±c hiá»‡n:
- **Cáº¥u hÃ¬nh random seed**: GiÃºp káº¿t quáº£ nháº¥t quÃ¡n.  
- **Import cÃ¡c thÆ° viá»‡n liÃªn quan:** Lightning vÃ  Torchmetrics.  
"""

# ============================================================
# 2) SETUP PYTORCH LIGHTNING
# ============================================================
import pytorch_lightning as pl
pl.seed_everything(42)

import torchmetrics
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import CSVLogger

""" ## **3. Táº£i Dá»¯ Liá»‡u CIFAR-10 vÃ  Xá»­ LÃ½ Dá»¯ Liá»‡u**

 # ğŸ“Š Táº£i Dá»¯ Liá»‡u CIFAR-10 vÃ  Tiá»n Xá»­ LÃ½
Má»¥c tiÃªu:  
- Táº£i táº­p dá»¯ liá»‡u CIFAR-10 (áº£nh 32x32 vá»›i 10 lá»›p).  
- Thá»±c hiá»‡n cÃ¡c phÃ©p biáº¿n Ä‘á»•i dá»¯ liá»‡u (Data Augmentation).  

### CÃ¡c bÆ°á»›c chÃ­nh:
1. **Táº£i CIFAR-10**: Tá»± Ä‘á»™ng táº£i náº¿u chÆ°a cÃ³.  
2. **Tiá»n xá»­ lÃ½:**
   - **Train Transform:**  
     - Cáº¯t ngáº«u nhiÃªn, láº­t ngang, xoay áº£nh.  
   - **Test Transform:**  
     - Chá»‰ resize vÃ  chuáº©n hÃ³a.  
3. **Chia tÃ¡ch dá»¯ liá»‡u:**  
   - Táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra.  
"""

# ============================================================
# 3) HÃ€M Táº¢I & Táº O DATALOADER CHO CIFAR-10
# ============================================================
def prepare_cifar10_data(data_dir='/content/drive/MyDrive/cifar_data', batch_size=128):
    """
    - Táº£i CIFAR-10 vÃ  lÆ°u vÃ o data_dir (náº¿u chÆ°a cÃ³).
    - Tráº£ vá» train_loader, test_loader, train_dataset, test_dataset.
    """
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)

    print("=> Táº£i & lÆ°u dá»¯ liá»‡u CIFAR-10 táº¡i:", data_dir)

    # Transform cho train
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                             (0.2470, 0.2435, 0.2616))
    ])

    # Transform cho test
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                             (0.2470, 0.2435, 0.2616))
    ])

    train_dataset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=train_transform)
    test_dataset  = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=test_transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=2)

    # Liá»‡t kÃª thÆ° má»¥c
    print("Cáº¥u trÃºc thÆ° má»¥c CIFAR-10:")
    !ls -R {data_dir}

    return train_loader, test_loader, train_dataset, test_dataset

# Gá»i hÃ m vÃ  táº£i dá»¯ liá»‡u
train_loader, val_loader, train_dataset, val_dataset = prepare_cifar10_data(
    data_dir='/content/drive/MyDrive/cifar_data',
    batch_size=128
)

print("Sá»‘ lÆ°á»£ng áº£nh train:", len(train_dataset))
print("Sá»‘ lÆ°á»£ng áº£nh test:",  len(val_dataset))
class_names = train_dataset.classes
print("Lá»›p CIFAR-10:", class_names)

"""### **3.1. Trá»±c quan hÃ³a táº­p dá»¯ liá»‡u**"""

# ============================================================
# 3.1) TRá»°C QUAN HÃ“A PHÃ‚N Bá» Dá»® LIá»†U TRAIN & TEST
# ============================================================

# Láº¥y sá»‘ lÆ°á»£ng máº«u theo lá»›p
train_counts = [train_dataset.targets.count(i) for i in range(len(train_dataset.classes))]
val_counts = [val_dataset.targets.count(i) for i in range(len(val_dataset.classes))]

# Táº¡o DataFrame Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“
df_train = pd.DataFrame({"Class": train_dataset.classes, "Count": train_counts})
df_val = pd.DataFrame({"Class": val_dataset.classes, "Count": val_counts})

# Táº¡o má»™t figure vá»›i 2 biá»ƒu Ä‘á»“ cáº¡nh nhau
fig, axs = plt.subplots(1, 2, figsize=(18, 6))

# Biá»ƒu Ä‘á»“ táº­p Train (Cáº­p nháº­t Ä‘á»ƒ trÃ¡nh cáº£nh bÃ¡o)
sns.barplot(y="Class", x="Count", data=df_train, hue="Class", palette="Blues", ax=axs[0], legend=False)
axs[0].set_title("PhÃ¢n bá»‘ dá»¯ liá»‡u Train CIFAR-10")
axs[0].set_xlabel("Sá»‘ lÆ°á»£ng máº«u")
axs[0].set_ylabel("Lá»›p")

# Biá»ƒu Ä‘á»“ táº­p Test (Cáº­p nháº­t Ä‘á»ƒ trÃ¡nh cáº£nh bÃ¡o)
sns.barplot(y="Class", x="Count", data=df_val, hue="Class", palette="Greens", ax=axs[1], legend=False)
axs[1].set_title("PhÃ¢n bá»‘ dá»¯ liá»‡u Test CIFAR-10")
axs[1].set_xlabel("Sá»‘ lÆ°á»£ng máº«u")
axs[1].set_ylabel("Lá»›p")

# Tinh chá»‰nh lá» vÃ  hiá»ƒn thá»‹
plt.tight_layout()
plt.show()

"""### 3.2. **Hiá»ƒn thá»‹ áº£nh trong lá»›p**"""

# ============================================================
# 3.2) HIá»‚N THá»Š áº¢NH THEO Lá»šP ÄÆ¯á»¢C NGÆ¯á»œI DÃ™NG CHá»ŒN
# ============================================================

# HÃ m Ä‘áº£o ngÆ°á»£c Normalize (Ä‘á»ƒ hiá»ƒn thá»‹ áº£nh gá»‘c)
def denormalize(img, mean=(0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616)):
    img = img.clone().permute(1, 2, 0).numpy()
    img = img * std + mean
    img = np.clip(img, 0, 1)  # Äáº£m báº£o giÃ¡ trá»‹ trong khoáº£ng [0, 1]
    return img

# Dropdown Ä‘á»ƒ ngÆ°á»i dÃ¹ng chá»n lá»›p vÃ  hiá»ƒn thá»‹ áº£nh
def display_images_for_class(class_name):
    """
    HÃ m hiá»ƒn thá»‹ 4 áº£nh ngáº«u nhiÃªn tá»« lá»›p Ä‘Æ°á»£c ngÆ°á»i dÃ¹ng chá»n.
    """
    class_idx = train_dataset.classes.index(class_name)
    # Láº¥y táº¥t cáº£ cÃ¡c chá»‰ sá»‘ áº£nh thuá»™c lá»›p Ä‘Æ°á»£c chá»n
    indices = [i for i, label in enumerate(train_dataset.targets) if label == class_idx]

    # Chá»n 8 áº£nh ngáº«u nhiÃªn tá»« lá»›p
    selected_indices = random.sample(indices, 4)

    # Hiá»ƒn thá»‹ áº£nh
    plt.figure(figsize=(12, 8))
    for i, idx in enumerate(selected_indices):
        img, label = train_dataset[idx]
        img = denormalize(img)
        plt.subplot(2, 4, i + 1)
        plt.imshow(img)
        plt.title(f"Lá»›p: {train_dataset.classes[label]}")
        plt.axis("off")
    plt.tight_layout()
    plt.show()

# Táº¡o dropdown tÆ°Æ¡ng tÃ¡c cho phÃ©p ngÆ°á»i dÃ¹ng chá»n lá»›p
interact(display_images_for_class, class_name=Dropdown(options=train_dataset.classes))

"""## **4. XÃ¢y dá»±ng kiáº¿n trÃºc Densenet121**

## ğŸ“¦ **Cáº¥u TrÃºc Tá»•ng Quan:**
- **Input:** áº¢nh 32x32 (CIFAR-10).  
- **Growth Rate:** 32  
- **Block Configuration:** (6, 12, 24, 16)  
- **4 DenseBlock + 3 Transition Layer.**  
- **Global Average Pooling â†’ Linear Classifier (10 lá»›p)**.  

## âœ… **TÃ³m Táº¯t:**
- **DenseLayer:** Táº¡o feature vÃ  káº¿t há»£p vá»›i Ä‘áº§u vÃ o.  
- **DenseBlock:** Káº¿t ná»‘i dÃ y Ä‘áº·c cÃ¡c `DenseLayer`.  
- **Transition Layer:** Giáº£m kÃ­ch thÆ°á»›c vÃ  kÃªnh.  
- **DenseNetCIFAR10:** Káº¿t há»£p Ä‘áº§y Ä‘á»§ cÃ¡c thÃ nh pháº§n Ä‘á»ƒ huáº¥n luyá»‡n trÃªn CIFAR-10.  

"""

# ============================================================
# 5) XÃ‚Y Dá»°NG KIáº¾N TRÃšC DENSENET-121
# ============================================================
from collections import OrderedDict

class _DenseLayer(nn.Module):
    """
    Má»—i DenseLayer: BN->ReLU->Conv(1x1) -> BN->ReLU->Conv(3x3), concat
    """
    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):
        super().__init__()
        self.norm1 = nn.BatchNorm2d(num_input_features)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate,
                               kernel_size=1, stride=1, bias=False)

        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)
        self.relu2 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate,
                               kernel_size=3, stride=1, padding=1, bias=False)

        self.drop_rate = drop_rate

    def forward(self, x):
        out = self.conv1(self.relu1(self.norm1(x)))
        out = self.conv2(self.relu2(self.norm2(out)))
        if self.drop_rate > 0:
            out = F.dropout(out, p=self.drop_rate, training=self.training)
        return torch.cat([x, out], 1)

class _DenseBlock(nn.Module):
    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):
        super().__init__()
        layers = []
        for i in range(num_layers):
            layer = _DenseLayer(
                num_input_features + i * growth_rate,
                growth_rate=growth_rate,
                bn_size=bn_size,
                drop_rate=drop_rate
            )
            layers.append(layer)
        self.layers = nn.ModuleList(layers)

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

class _Transition(nn.Sequential):
    """
    Transition: BN->ReLU->Conv(1x1) -> AvgPool(2x2)
    """
    def __init__(self, num_input_features, num_output_features):
        super().__init__()
        self.add_module("norm", nn.BatchNorm2d(num_input_features))
        self.add_module("relu", nn.ReLU(inplace=True))
        self.add_module("conv", nn.Conv2d(num_input_features, num_output_features,
                                          kernel_size=1, stride=1, bias=False))
        self.add_module("pool", nn.AvgPool2d(kernel_size=2, stride=2))

class DenseNetCIFAR10(nn.Module):
    """
    DenseNet-121 cho CIFAR-10 (32x32)
    """
    def __init__(self, growth_rate=32, block_config=(6,12,24,16),
                 num_init_features=64, bn_size=4, drop_rate=0.0, num_classes=10):
        super().__init__()

        # Stem (Khá»Ÿi Ä‘áº§u máº¡ng)
        self.features = nn.Sequential(OrderedDict([
            ("conv0", nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),
            ("norm0", nn.BatchNorm2d(num_init_features)),
            ("relu0", nn.ReLU(inplace=True)),
            ("pool0", nn.MaxPool2d(kernel_size=2, stride=2))
        ]))

        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            block = _DenseBlock(num_layers, num_features, bn_size, growth_rate, drop_rate)
            self.features.add_module(f"denseblock{i+1}", block)
            num_features = num_features + num_layers * growth_rate
            if i != len(block_config) - 1:
                trans = _Transition(num_features, num_features // 2)
                self.features.add_module(f"transition{i+1}", trans)
                num_features = num_features // 2

        # BatchNorm Cuá»‘i + Classifier
        self.features.add_module("norm_final", nn.BatchNorm2d(num_features))
        self.classifier = nn.Linear(num_features, num_classes)

    def forward(self, x):
        out = self.features(x)
        out = F.relu(out, inplace=True)
        out = F.adaptive_avg_pool2d(out, (1, 1))
        out = torch.flatten(out, 1)
        out = self.classifier(out)
        return out

"""## **5. Äá»‹nh NghÄ©a LitDenseNet**

## ğŸ¯ **Má»¥c TiÃªu:**
- XÃ¢y dá»±ng lá»›p `LitDenseNet` sá»­ dá»¥ng **PyTorch Lightning**.  
- TÃ­ch há»£p toÃ n bá»™ quy trÃ¬nh huáº¥n luyá»‡n, validation vÃ  test trong má»™t class duy nháº¥t.  
- Äáº£m báº£o kháº£ nÄƒng huáº¥n luyá»‡n dá»… theo dÃµi vÃ  má»Ÿ rá»™ng.  


## ğŸ“¦ **Cáº¥u TrÃºc Tá»•ng Quan:**
- **MÃ´ HÃ¬nh:** Sá»­ dá»¥ng `DenseNetCIFAR10` Ä‘Ã£ xÃ¢y dá»±ng trÆ°á»›c Ä‘Ã³.  
- **CÃ¡c ThÃ nh Pháº§n ChÃ­nh:**
   - `configure_optimizers`: Äá»‹nh nghÄ©a optimizer vÃ  scheduler.  
   - `training_step`: TÃ­nh toÃ¡n loss vÃ  accuracy trong huáº¥n luyá»‡n.  
   - `validation_step`: TÃ­nh toÃ¡n loss vÃ  accuracy trong validation.  
   - `test_step`: TÃ­nh toÃ¡n loss vÃ  accuracy trong kiá»ƒm tra.  
   - **Logging:** Log loss vÃ  accuracy sau má»—i 100 step.
"""

# ============================================================
# 6) LITDENSENET CHO LIGHTNING >= 2.0 (CHá»ˆ LOG THEO EPOCH)
# ============================================================
class LitDenseNet(pl.LightningModule):
    def __init__(self, num_classes=10):
        super().__init__()
        self.save_hyperparameters()

        # Khá»Ÿi táº¡o mÃ´ hÃ¬nh DenseNet CIFAR-10
        self.model = DenseNetCIFAR10(
            growth_rate=32,
            block_config=(6,12,24,16),
            num_init_features=64,
            bn_size=4,
            drop_rate=0.0,
            num_classes=num_classes
        )

        # Metric sá»­ dá»¥ng trong training vÃ  validation
        self.train_acc_metric = torchmetrics.Accuracy(task="multiclass", num_classes=num_classes)
        self.val_acc_metric   = torchmetrics.Accuracy(task="multiclass", num_classes=num_classes)

        # Thiáº¿t láº­p log má»—i 100 step
        self.log_interval = 100
        self.train_step_count = 0

        # Äá»ƒ lÆ°u lá»‹ch sá»­ train cho file JSON
        self.history = []

        # LÆ°u trá»¯ táº¡m loss & acc cho trung bÃ¬nh cuá»‘i epoch
        self.train_losses = []
        self.train_accs   = []

    def forward(self, x):
        return self.model(x)

    # ===========================
    # Cáº¥u hÃ¬nh tá»‘i Æ°u hÃ³a
    # ===========================
    def configure_optimizers(self):
        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
        return [optimizer], [scheduler]

    # ===========================
    # TRAINING
    # ===========================
    def on_train_epoch_start(self):
        self.train_losses = []
        self.train_accs   = []

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        acc = self.train_acc_metric(y_hat, y)

        # ThÃªm vÃ o danh sÃ¡ch lÆ°u táº¡m Ä‘á»ƒ tÃ­nh trung bÃ¬nh cuá»‘i epoch
        self.train_losses.append(loss.item())
        self.train_accs.append(acc.item())

        # Log má»—i 100 step (chá»‰ hiá»ƒn thá»‹)
        self.train_step_count += 1
        if self.train_step_count % self.log_interval == 0:
            print(f"BÆ°á»›c [{self.train_step_count}/{len(self.trainer.train_dataloader)}], Loss: {loss.item():.4f}")

        return loss

    def on_train_epoch_end(self):
        # TÃ­nh trung bÃ¬nh Loss & Accuracy cuá»‘i epoch
        avg_loss = float(np.mean(self.train_losses))
        avg_acc  = float(np.mean(self.train_accs))
        self.log('train_loss_epoch', avg_loss, on_epoch=True, prog_bar=True)
        self.log('train_acc_epoch', avg_acc, on_epoch=True, prog_bar=True)

        # In káº¿t quáº£ cuá»‘i epoch
        print(f"Epoch [{self.current_epoch+1}/{self.trainer.max_epochs}], Loss huáº¥n luyá»‡n: {avg_loss:.4f}")
        self.train_acc_metric.reset()

    # ===========================
    # VALIDATION
    # ===========================
    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        acc = self.val_acc_metric(y_hat, y)
        self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log("val_acc", acc, on_step=False, on_epoch=True, prog_bar=True)
        return loss

    def on_validation_epoch_end(self):
        current_epoch = self.current_epoch + 1
        train_loss_epoch = float(self.trainer.callback_metrics.get("train_loss_epoch", 0.0))
        val_loss_epoch   = float(self.trainer.callback_metrics.get("val_loss", 0.0))
        val_acc_epoch    = float(self.trainer.callback_metrics.get("val_acc", 0.0))

        # Hiá»ƒn thá»‹ káº¿t quáº£ cuá»‘i epoch
        print(f"Loss kiá»ƒm tra: {val_loss_epoch:.4f}, Äá»™ chÃ­nh xÃ¡c: {val_acc_epoch*100:.2f}%")

        # Ghi vÃ o lá»‹ch sá»­ huáº¥n luyá»‡n
        self.history.append({
            "epoch": current_epoch,
            "train_loss": train_loss_epoch,
            "val_loss": val_loss_epoch,
            "val_acc": val_acc_epoch
        })

        self.val_acc_metric.reset()

    # ===========================
    # TESTING
    # ===========================
    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        acc = self.val_acc_metric(y_hat, y)
        self.log('test_loss', loss, on_step=False, on_epoch=True)
        self.log('test_acc', acc, on_step=False, on_epoch=True)
        return loss

    def on_test_epoch_end(self):
        self.val_acc_metric.reset()

"""## **6. Huáº¥n Luyá»‡n MÃ´ HÃ¬nh DenseNet-121**

## ğŸ¯ **Má»¥c TiÃªu:**
- Huáº¥n luyá»‡n mÃ´ hÃ¬nh **DenseNet-121** trÃªn táº­p dá»¯ liá»‡u **CIFAR-10** trong **100 epoch**.  
- **KhÃ´ng sá»­ dá»¥ng EarlyStopping**.  
- Chá»‰ lÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t dá»±a trÃªn **Validation Accuracy** (`val_acc`).  

## ğŸ“¦ **Chi Tiáº¿t Cáº¥u HÃ¬nh Huáº¥n Luyá»‡n:**
- **Epoch:** 100  
- **Batch Size:** 128  
- **Optimizer:** SGD vá»›i Momentum = 0.9  
- **Loss Function:** CrossEntropy  
- **KhÃ´ng sá»­ dá»¥ng:** EarlyStopping, CSVLogger.  
- **Log:** Ghi log má»—i **100 step**.  
- **Checkpoint:** LÆ°u **mÃ´ hÃ¬nh tá»‘t nháº¥t** khi `val_acc` Ä‘áº¡t cao nháº¥t.  
"""

# ============================================================
# 7) HUáº¤N LUYá»†N 100 EPOCH, LÆ¯U CHECKPOINT VÃ€ JSON
# ============================================================

# Callback chá»‰ lÆ°u checkpoint dá»±a trÃªn val_acc cao nháº¥t
checkpoint_callback = ModelCheckpoint(
    monitor='val_acc',
    dirpath=base_dir,
    filename='best_densenet_cifar10',
    save_top_k=1,
    mode='max'
)

# Táº¡o Trainer vá»›i log má»—i 100 step, KHÃ”NG cÃ³ EarlyStopping
trainer = pl.Trainer(
    max_epochs=100,                  # Huáº¥n luyá»‡n 100 epoch
    accelerator="auto",
    callbacks=[checkpoint_callback], # Chá»‰ cÃ³ checkpoint callback
    log_every_n_steps=100,           # Log má»—i 100 step
    enable_progress_bar=False        # Táº¯t progress bar, chá»‰ log text má»—i 100 step
)

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh
lit_model = LitDenseNet(num_classes=10)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
trainer.fit(lit_model, train_loader, val_loader)

print("=> QuÃ¡ trÃ¬nh huáº¥n luyá»‡n hoÃ n táº¥t!")
print(f"=> ÄÆ°á»ng dáº«n model tá»‘t nháº¥t: {checkpoint_callback.best_model_path}")

# LÆ°u lá»‹ch sá»­ huáº¥n luyá»‡n vÃ o file JSON
history_path = os.path.join(base_dir, 'history_densenet_cifar10.json')
with open(history_path, 'w') as f:
    json.dump(lit_model.history, f, indent=2)

print(f"=> ÄÃ£ lÆ°u lá»‹ch sá»­ huáº¥n luyá»‡n vÃ o: {history_path}")

"""## **7. Kiá»ƒm Thá»­ MÃ´ HÃ¬nh DenseNet-121**

## ğŸ¯ **Má»¥c TiÃªu:**
- Kiá»ƒm thá»­ mÃ´ hÃ¬nh **DenseNet-121** Ä‘Ã£ huáº¥n luyá»‡n.  
- Cho phÃ©p ngÆ°á»i dÃ¹ng **táº£i áº£nh lÃªn** vÃ  mÃ´ hÃ¬nh sáº½ dá»± Ä‘oÃ¡n lá»›p cá»§a áº£nh Ä‘Ã³.  
- KhÃ´ng hiá»ƒn thá»‹ pháº§n trÄƒm dá»± Ä‘oÃ¡n, chá»‰ hiá»ƒn thá»‹ **lá»›p dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng**.

## ğŸ“¦ **Quy TrÃ¬nh Thá»±c Hiá»‡n:**
1. **Táº£i MÃ´ HÃ¬nh Tá»« Checkpoint:**  
   - Táº£i mÃ´ hÃ¬nh Ä‘Ã£ lÆ°u (`.ckpt`).  
2. **Tiá»n Xá»­ LÃ½ áº¢nh:**  
   - Resize áº£nh vá» **32x32** (chuáº©n CIFAR-10).  
   - Chuáº©n hÃ³a áº£nh giá»‘ng nhÆ° khi huáº¥n luyá»‡n.  
3. **NgÆ°á»i DÃ¹ng Táº£i áº¢nh LÃªn:**  
   - NgÆ°á»i dÃ¹ng cÃ³ thá»ƒ táº£i áº£nh trá»±c tiáº¿p tá»« mÃ¡y tÃ­nh.  
4. **Dá»± ÄoÃ¡n:**  
   - MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n vÃ  tráº£ vá» **nhÃ£n Ä‘á»‘i tÆ°á»£ng**.  
5. **Hiá»ƒn Thá»‹ Káº¿t Quáº£:**  
   - Hiá»ƒn thá»‹ áº£nh kÃ¨m **káº¿t quáº£ nháº­n dáº¡ng Ä‘á»‘i tÆ°á»£ng**.  
"""

# ============================================================
# 8) KIá»‚M THá»¬ MÃ” HÃŒNH DENSENET-121 Tá»ª FILE .pt (XÃ“A Káº¾T QUáº¢ CÅ¨)
# ============================================================

# ÄÆ°á»ng dáº«n model tá»‘t nháº¥t Ä‘Ã£ lÆ°u tá»« trÆ°á»›c
best_model_path = "/content/drive/MyDrive/best_densenet_cifar10.ckpt"

# Kiá»ƒm tra thiáº¿t bá»‹ vÃ  sá»­ dá»¥ng GPU náº¿u cÃ³
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"=> Äang sá»­ dá»¥ng thiáº¿t bá»‹: {device}")

# ==========================
# HÃ€M Táº¢I MÃ” HÃŒNH Tá»ª CHECKPOINT
# ==========================
def load_model_from_checkpoint(checkpoint_path):
    model = LitDenseNet.load_from_checkpoint(checkpoint_path)
    model.to(device)  # ÄÆ°a model lÃªn Ä‘Ãºng thiáº¿t bá»‹
    model.eval()
    return model

# Táº£i mÃ´ hÃ¬nh tá»« checkpoint Ä‘Ã£ lÆ°u
model = load_model_from_checkpoint(best_model_path)
print(f"=> ÄÃ£ táº£i mÃ´ hÃ¬nh tá»«: {best_model_path}")

# ==========================
# HÃ€M TIá»€N Xá»¬ LÃ áº¢NH Äá»‚ Dá»° ÄOÃN
# ==========================
def preprocess_image(image_path):
    transform = transforms.Compose([
        transforms.Resize((32, 32)),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                             (0.2470, 0.2435, 0.2616))
    ])
    image = Image.open(image_path).convert("RGB")
    input_tensor = transform(image).unsqueeze(0).to(device)  # ÄÆ°a lÃªn thiáº¿t bá»‹
    return input_tensor

# ==========================
# HÃ€M Dá»° ÄOÃN áº¢NH
# ==========================
def predict_image(model, image_tensor, class_names):
    with torch.no_grad():
        outputs = model(image_tensor)
        _, predicted_class = torch.max(outputs, 1)
        predicted_label = class_names[predicted_class.item()]
    return predicted_label

# ==========================
# Táº O BUTTON VÃ€ Cáº¬P NHáº¬T Káº¾T QUáº¢ Dá»° ÄOÃN
# ==========================
def on_file_upload(change):
    # XÃ³a káº¿t quáº£ cÅ© trÆ°á»›c khi hiá»ƒn thá»‹ káº¿t quáº£ má»›i
    clear_output(wait=True)
    display(upload_button)

    for filename in change['new']:
        # LÆ°u file táº¡m
        with open(filename, 'wb') as f:
            f.write(change['new'][filename]['content'])
        print(f"=> áº¢nh Ä‘Ã£ chá»n: {filename}")

        # Tiá»n xá»­ lÃ½ vÃ  dá»± Ä‘oÃ¡n áº£nh
        image_tensor = preprocess_image(filename)
        predicted_class = predict_image(model, image_tensor, train_dataset.classes)

        # Hiá»ƒn thá»‹ áº£nh vÃ  káº¿t quáº£ dá»± Ä‘oÃ¡n má»›i nháº¥t
        plt.figure(figsize=(6,6))
        img = Image.open(filename)
        plt.imshow(img)
        plt.title(f"Dá»± Ä‘oÃ¡n: {predicted_class}")
        plt.axis('off')
        plt.show()

# Táº¡o button chá»n file vÃ  láº¯ng nghe sá»± kiá»‡n
upload_button = widgets.FileUpload(accept='image/*', multiple=False)
upload_button.observe(on_file_upload, names='value')
display(upload_button)

"""## **8. Trá»±c Quan HÃ³a Káº¿t Quáº£ Huáº¥n Luyá»‡n**

## ğŸ¯ **Má»¥c TiÃªu:**
- Trá»±c quan hÃ³a káº¿t quáº£ huáº¥n luyá»‡n cá»§a mÃ´ hÃ¬nh **DenseNet-121**.  
- So sÃ¡nh trá»±c tiáº¿p giá»¯a **Train** vÃ  **Validation**.  
- Táº­p trung vÃ o 2 chá»‰ sá»‘ chÃ­nh:  
   - **Loss:** Äá»™ lá»—i giáº£m dáº§n theo quÃ¡ trÃ¬nh huáº¥n luyá»‡n.  
   - **Accuracy:** Äá»™ chÃ­nh xÃ¡c tÄƒng dáº§n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.  

## ğŸ“¦ **Dá»¯ Liá»‡u Äáº§u VÃ o:**
- **File:** `history_densenet_cifar10.json`.  
- **Dá»¯ Liá»‡u Chá»©a:**  
   - `train_loss`, `val_loss`.  
   - `train_acc`, `val_acc`.  
   - `epoch`: Sá»‘ epoch huáº¥n luyá»‡n.  

## ğŸ“Š **Biá»ƒu Äá»“ Hiá»ƒn Thá»‹:**
- **Cá»™t 1:** So sÃ¡nh **Train Accuracy** vÃ  **Validation Accuracy**.  
- **Cá»™t 2:** So sÃ¡nh **Train Loss** vÃ  **Validation Loss**.  
- **Äáº·c Äiá»ƒm:**
   - ÄÆ°á»£c chia thÃ nh **2 biá»ƒu Ä‘á»“** Ä‘áº·t cáº¡nh nhau.  
   - Dá»… dÃ ng so sÃ¡nh trá»±c tiáº¿p giá»¯a táº­p huáº¥n luyá»‡n vÃ  kiá»ƒm tra.  
"""

# ============================================================
# 9) TRá»°C QUAN HÃ“A Tá»I Æ¯U (VAL_ACC & VAL_LOSS) + ÄÃNH Dáº¤U ÄIá»‚M CAO NHáº¤T
# ============================================================

# ÄÆ°á»ng dáº«n tá»›i file JSON Ä‘Ã£ lÆ°u lá»‹ch sá»­ huáº¥n luyá»‡n
history_path = "/content/drive/MyDrive/history_densenet_cifar10.json"

# Äá»c file JSON chá»©a lá»‹ch sá»­ huáº¥n luyá»‡n
with open(history_path, 'r') as f:
    history_data = json.load(f)

# Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u JSON thÃ nh DataFrame Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“ dá»… hÆ¡n
df_history = pd.DataFrame(history_data)

# ==========================
# TÃŒM ÄIá»‚M VAL_ACC CAO NHáº¤T
# ==========================
max_val_acc = df_history['val_acc'].max()
best_epoch = df_history[df_history['val_acc'] == max_val_acc]['epoch'].values[0]

print(f"ğŸ¯ Val Acc cao nháº¥t: {max_val_acc:.4f} táº¡i epoch {best_epoch}")

# ==========================
# Tá»I Æ¯U TRá»°C QUAN HÃ“A
# ==========================
# Giáº£m kÃ­ch thÆ°á»›c biá»ƒu Ä‘á»“ 10% vÃ  thÃªm khoáº£ng cÃ¡ch giá»¯a cÃ¡c biá»ƒu Ä‘á»“
fig, axes = plt.subplots(1, 2, figsize=(14.4, 5.4))  # Giáº£m tá»« (16,6) -> (14.4, 5.4)
plt.subplots_adjust(wspace=0.3)  # TÄƒng khoáº£ng cÃ¡ch giá»¯a 2 biá»ƒu Ä‘á»“

# Biá»ƒu Ä‘á»“ Validation Accuracy (CÃ³ Äiá»ƒm ÄÃ¡nh Dáº¥u)
sns.lineplot(ax=axes[0], x="epoch", y="val_acc", data=df_history, label="Validation Accuracy", marker='o', color='blue')
axes[0].axvline(best_epoch, color='red', linestyle='--', linewidth=2, label=f"Best Epoch ({best_epoch})")
axes[0].scatter(best_epoch, max_val_acc, color='red', s=150)  # ÄÃ¡nh dáº¥u Ä‘iá»ƒm cao nháº¥t
axes[0].set_title("Validation Accuracy", fontsize=16)
axes[0].set_xlabel("Epoch", fontsize=12)
axes[0].set_ylabel("Accuracy", fontsize=12)
axes[0].legend(fontsize=10)
axes[0].grid(color='gray', linestyle='--', linewidth=0.5)

# Biá»ƒu Ä‘á»“ Validation Loss
sns.lineplot(ax=axes[1], x="epoch", y="val_loss", data=df_history, label="Validation Loss", marker='o', color='orange')
axes[1].set_title("Validation Loss", fontsize=16)
axes[1].set_xlabel("Epoch", fontsize=12)
axes[1].set_ylabel("Loss", fontsize=12)
axes[1].legend(fontsize=10)
axes[1].grid(color='gray', linestyle='--', linewidth=0.5)

# Hiá»ƒn thá»‹ biá»ƒu Ä‘á»“
plt.tight_layout()
plt.show()